\documentclass{article}
\usepackage{import}
\usepackage[ruled]{algorithm2e}
\usepackage[shortlabels]{enumitem}
\subimport*{../}{macro}

%Chloe: added commands below
\newcommand*{\RN}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\providecommand*{\eu}{\ensuremath{\mathrm{e}}}



\setlength\parindent{0px}

\begin{document}
\setcounter{aprob}{0}
\title{Homework \#3}
\author{
    \normalsize{CSE 446/546: Machine Learning}\\
    \normalsize{Profs. Jamie Morgenstern and Ludwig Schmidt}\\
    \normalsize{Due: \textbf{Wednesday} Nov 23, 2022 11:59pm} \\
    \normalsize{\textbf{A}: 101 points, \textbf{B}: 16 points}
}
\date{{}}
\maketitle

\noindent Please review all homework guidance posted on the website before submitting it to GradeScope. Reminders:
\begin{itemize}
    \item Make sure to read the ``What to Submit'' section following each question and include all items.
    \item Please provide succinct answers and supporting reasoning for each question. Similarly, when discussing experimental results, concisely create tables and/or figures when appropriate to organize the experimental results. All explanations, tables, and figures for any particular part of a question must be grouped together.
    \item For every problem involving generating plots, please include the plots as part of your PDF submission.
    \item When submitting to Gradescope, please link each question from the homework in Gradescope to the location of its answer in your homework PDF. Failure to do so may result in deductions of up to 10\% of the value of each question not properly linked. For instructions, see \url{https://www.gradescope.com/get_started#student-submission}.
    \item If you collaborate on this homework with others, you must indicate who you worked with on your homework by providing a complete list of collaborators on the first page of your assignment. Make sure to include the name of each collaborator, and on which problem(s) you collaborated. Failure to do so may result in accusations of plagiarism. You can review the course collaboration policy at \url{https://courses.cs.washington.edu/courses/cse446/22au/assignments/}
    \item For every problem involving code, please include all code you have written for the problem as part of your PDF submission \emph{in addition to} submitting your code to the separate assignment on Gradescope created for code. Not submitting all code files will lead to a deduction of up to 10\% of the value of each question missing code.  
\end{itemize}

Not adhering to these reminders may result in point deductions. \\

\clearpage{}

\section*{Conceptual Questions}

\begin{aprob}
    The answers to these questions should be answerable without referring to external materials.
    Briefly justify your answers with a few words.
    \begin{enumerate}
        \item \points{2} Say you trained an SVM classifier with an RBF kernel ($K(u, v) = \exp\left(-\frac{\left\|u-v\right\|^2_2}{2\sigma^2}\right)$). It seems to underfit the training set: should you increase or decrease $\sigma$?
        \item \points{2} True or False:   Training deep neural networks requires minimizing a non-convex loss function, and therefore gradient descent might not reach the globally-optimal solution.
        \item \points{2} True or False: It is a good practice to initialize all weights to zero when training a deep neural network.
        \item \points{2} True or False:   We use non-linear activation functions in a neural networkâ€™s hidden layers so that the network learns non-linear decision boundaries.
        \item \points{2} True or False: Given a neural network, the time complexity of the backward pass step in the backpropagation algorithm can be prohibitively larger compared to the relatively low time complexity of the forward pass step.
        \item \points{2} True or False: Neural Networks are the most extensible model and therefore the best choice for any circumstance.
    \end{enumerate}
    
    \subsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Part a-f:} 1-2 sentence explanation containing your answer.
    \end{itemize}
\end{aprob}

\section*{Logistic Regression}
% \subsection*{Binary Logistic Regression} 
\begin{aprob}
    Here we consider the MNIST dataset, but for binary classification. Specifically, the task is to determine whether a digit is a $2$ or $7$.
    Here, let $Y=1$ for all the ``7'' digits in the dataset, and use $Y=-1$ for ``2''.
    We will use regularized logistic regression. 
    Given a binary classification dataset $\{(x_i,y_i)\}_{i=1}^n$ for $x_i \in \R^d$ and $y_i \in \{-1,1\}$ we showed in class that the regularized negative log likelihood objective function can be written as
    \begin{align*}
    J(w,b) = \frac{1}{n} \sum_{i=1}^n \log( 1 + \exp(-y_i (b + x_i^T w))) + \lambda ||w||_2^2
    \end{align*} 
    Note that the offset term $b$ is not regularized. 
    For all experiments, use $\lambda = 10^{-1}$. 
    Let $\mu_i(w,b) = \frac{1}{1+ \exp(-y_i (b + x_i^T w))}$. 
    \begin{enumerate}
        \item \points{8} Derive the gradients $\nabla_w J(w,b)$, $\nabla_{b} J(w,b)$ and give your answers in terms of $\mu_i(w,b)$ (your answers should not contain exponentials).
        \item \points{8} Implement gradient descent with an initial iterate of all zeros. Try several values of step sizes to find one that appears to make convergence on the training set as fast as possible. Run until you feel you are near to convergence.
        \begin{enumerate}[(i)]
            \item For both the training set and the test, plot $J(w,b)$ as a function of the iteration number (and show both curves on the same plot).  
            \item For both the training set and the test, classify the points according to the rule $\text{sign}(b + x_i^T w)$ and plot the misclassification error as a function of the iteration number (and show both curves on the same plot). 
        \end{enumerate}
          
        Reminder: Make sure you are only using the test set for evaluation (not for training).
          
        \item \points{7} Repeat (b) using stochastic gradient descent with a batch size of 1. Note, the expected gradient with respect to the random selection should be equal to the gradient found in part (a). Show both plots described in (b) when using batch size 1. Take careful note of how to scale the regularizer.
        \item \points{7} Repeat (b) using stochastic gradient descent with batch size of 100. That is, instead of approximating the gradient with a single example, use 100. Note, the expected gradient with respect to the random selection should be equal to the gradient found in part (a).
    \end{enumerate}
    
    \subsection*{What to Submit}
    \begin{itemize}
        \item \textbf{Part a:} Proof
        \item \textbf{Part b:} Separate plots for b(i) and b(ii).
        \item \textbf{Part c:} Separate plots for c which reproduce those from b(i) and b(ii) for this case.
        \item \textbf{Part d:} Separate plots for c which reproduce those from b(i) and b(ii) for this case.
        \item \textbf{Code} on Gradescope through coding submission.
    \end{itemize}
\end{aprob}

\section*{Support Vector Machines}

\begin{aprob}
    Recall that solving the SVM problem amounts to solving the following constrained optimization problem:
    \begin{center}
        Given data points $\mathcal{D} = \{(x_i, y_i)\}^n_{i=1}$ find
        \begin{align*}
            \min_{w, b} ||w||_2 \text{ subject to } y_i(x_i^Tw - b) \geq 1 \text{ for } i \in \{1, \dots, n\}
        \end{align*}
        where $x_i \in \R^d$, $y_i \in \{-1, 1\}$, and $w \in \R^d$.
    \end{center}
    Consider the following labeled data points:
    \begin{align*}
        \begin{bmatrix}
        1 & 2 \\
        1 & 3 \\
        2 & 3 \\
        3 & 4 \\
        \end{bmatrix} \text{ with label } y=-1 \text{ and } \begin{bmatrix}
        0 & 0.5 \\
        1 & 0 \\
        2 & 1 \\
        3 & 0 \\
        \end{bmatrix} \text{ with label } y=1
    \end{align*}
    \begin{enumerate}
        \item \points{2} Graph the data points above. Highlight the support vectors and write their coordinates. Draw the two parallel hyperplanes separating the two classes of data such that the distance between them is as large as possible. Draw the maximum-margin hyperplane. Write the equations describing these three hyperplanes using only $x, w, b$(that is without using any specific values). Draw $w$(it doesn't have to have the exact magnitude, but it should have the correct orientation).
        
        \item \points{2} For the data points above, find $w$ and $b$. \\
        
        \textbf{Hint}: Use the support vectors and the values $\{-1, 1\}$ to create a linear system of equations where the unknowns are $w_1, w_2$ and $b$.
    
    \end{enumerate}
    \subsubsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Part a:} Write down support vectors and equations. Graph the points, hyperplanes, and $w$.
        \item \textbf{Part b:} Solution and corresponding calculations.
    \end{itemize}
\end{aprob}

\begin{bprob}
    \begin{enumerate}
        \item \points{2} Let $w^\top x + b=0$ be the hyperplane generated by a certain SVM optimization. Given some point $x_0 \in \R^n$, Show that the minimum \emph{squared distance} to the hyperplane is $\frac{|x_0^\top w + b|}{\|w\|_2}$.
    	In other words, show the following:
    	\begin{align*}
        	\min_x \|x_0 - x\|_2 = \frac{|x_0^\top w + b|}{\|w\|_2}\\
        	\text{subject to: } w^\top x +b = 0\, .
    	\end{align*}
    	\textbf{Hint:} Think about projecting $x_0$ onto the hyperplane.
    	
        \item \points{4} Show that for any solvable SVM problem, the distance between the two separating hyperplanes is $\frac{2}{||w||_2}$. \\
        
        \textbf{Hint 1}: The distance between two hyperplanes is the distance between any point $x_0$ on one of the hyperplanes and its projection on the other hyperplane. \\
        \textbf{Hint 2}: A direction $w$ and an offset $c$ define the hyperplane: $H = \{x \in \R^n | w^Tx = c\}$. The projection of a vector $y$ onto $H$ is given by $P_H(y) = y - \frac{w^Ty - c}{||w||^2_2}w$.
    \end{enumerate}
    \subsubsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Part a-b:} Solution and corresponding calculations.
    \end{itemize}
\end{bprob}

\section*{Kernels}

\begin{aprob}
    \points{5} Suppose that our inputs $x$ are one-dimensional and that our feature map is infinite-dimensional: 
    $\phi( x) $ is a vector whose $i$th component is:
    $$\frac{1}{\sqrt{i!}} \eu^{-x^2/2}x^i\ ,$$
    for all nonnegative integers $i$. (Thus, $\phi$ is an infinite-dimensional vector.)
    \begin{enumerate}
        \item Show that $K(x, x') = \eu^{-\frac{(x-x')^2}{2}}$ is a kernel function for this feature map, i.e., 
        $$\phi (x) \cdot \phi (x') = \eu^{-\frac{(x-x')^2}{2}}\ .$$
        \textbf{Hint:} Use the Taylor expansion of $z \mapsto \eu^z$.
        (This is the one dimensional version of the Gaussian  (RBF) kernel).
        \iffalse
        \Yuanyuan{Commented this out.}
        \item Now, we are given a dataset of size $n$, i.e., $\{x_1,\ldots, x_n\}$, where each $x$ is one-dimensional. Let $G$ be the kernel matrix $G \in \R^{n\times n}$, where the $(i,j)$-th entry of $G$ is $G_{i,j} = K(x_i, x_j)$.
        Show that the kernel matrix is symmetric, and positive semi-definite, i.e.,
        \begin{align*}
            K &= K^\top \\
            x^\top K x &\geq 0 \text{ for any column vector }x 
        \end{align*}
        
        \textbf{Hint}: Matrix $K$ can be written as $K = \Phi(x)^\top \Phi(x)$, where matrix $\Phi := [\phi(x_1), \ldots, \phi(x_n)]$.
        \fi
    
        
        
    
    \end{enumerate}
    \subsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Part a} Solution and corresponding calculations. 
    \end{itemize}
\end{aprob}

\begin{aprob}
    This problem will get you familiar with kernel ridge regression using the polynomial and RBF kernels.
    First, let's generate some data. Let $n=30$ and $f_*(x) = 4 \sin(\pi x)\cos(6\pi x^2)$.
    For $i=1,\dots,n$ let each $x_i$ be drawn uniformly at random from $[0,1]$, and let $y_i = f_*(x_i) + \epsilon_i$ where $\epsilon_i \sim \mathcal{N}(0,1)$.
    For any function $f$, the true error and the train error are respectively defined as:
    $$\mathcal{E}_{\rm true}(f) = \mathbb{E}_{X,Y}\left[(f(X) - Y)^2\right], \quad \quad  \widehat{\mathcal{E}}_{\rm train}(f) =  \frac{1}{n} \sum_{i=1}^n \left(f(x_i)-y_i\right)^2.$$
    Now, our goal is, using kernel ridge regression, to construct a predictor:
    $$\widehat{\alpha} = \arg\min_\alpha \twonorm{K\alpha - y}^2 + \lambda \alpha^\top K \alpha \ , \quad \quad \widehat{f}(x) = \sum_{i=1}^n \widehat{\alpha}_i k(x_i,x)$$
    where $K\in\R^{n\times n}$ is the kernel matrix such that $K_{i,j} = k(x_i,x_j)$, and $\lambda\geq 0$ is the regularization constant.
    
    \begin{enumerate}
        \item \points{10} Using leave-one-out cross validation, find a good $\lambda$ and hyperparameter settings for the following kernels:
        \begin{itemize}
            \item $k_{\rm poly}(x,z) = (1+x^\top z)^d$ where $d \in \mathbb{N}$ is a hyperparameter, 
            \item $k_{\rm rbf}(x,z) = \exp(-\gamma \twonorm{x-z}^2)$ where $\gamma > 0$ is a hyperparameter\footnote{Given a dataset $x_1,\dots,x_n \in \R^d$, a heuristic for choosing a range of $\gamma$ in the right ballpark is the inverse of the median of all $\binom{n}{2}$ squared distances $\twonorm{x_i-x_j}^2$.}.
        \end{itemize}
        We strongly recommend implementing either \href{https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search}{grid search} or \href{https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search}{random search}. \textbf{Do not use sklearn}, but actually implement of these algorithms. Reasonable values to look through in this problem are: $\lambda \in 10^{[-5, -1]}$, $d \in [5, 25]$, $\gamma$ sampled from a narrow gaussian distribution centered at value described in the footnote. \\
        Report the values of $d$, $\gamma$, and the $\lambda$ values for both kernels.
        \item \points{10} Let $\widehat{f}_{\rm poly}(x)$ and $\widehat{f}_{\rm rbf}(x)$ be the functions learned using the hyperparameters you found in part a.
        For a single plot per function $\widehat{f} \in \left\lbrace \widehat{f}_{\rm poly}(x), \widehat{f}_{\rm rbf}(x) \right\rbrace$, plot the original data $\{(x_i,y_i)\}_{i=1}^n$, the true $f(x)$, and $\widehat{f}(x)$ (i.e., define a fine grid on $[0,1]$ to plot the functions).
        \iffalse \Yuanyuan{Commented out the bootstrap questions.}
        
        \item \points{5} We wish to build bootstrap percentile confidence intervals for $\widehat{f}_{\rm poly}(x)$ and $\widehat{f}_{\rm rbf}(x)$ for all $x \in [0,1]$ from part b.\footnote{See Hastie, Tibshirani, Friedman Ch. 8.2 for a review of the bootstrap procedure.}
        Use the non-parametric bootstrap with $B=300$ bootstrap iterations to find $5\%$ and $95\%$ percentiles at each point $x$ on a fine grid over $[0,1]$.
      
        \medskip
      
        Specifically, for each bootstrap sample $b \in \{1,\dots,B\}$, draw uniformly at randomly with replacement, $n$ samples from $\{(x_i,y_i)\}_{i=1}^n$, train an $\widehat{f}_b$ using the $b$-th resampled dataset, compute $\widehat{f}_b(x)$ for each $x$ in your fine grid; let the $5\%$ percentile at point $x$ be the largest value $\nu$ such that $\frac{1}{B} \sum_{b=1}^B \1\{ \widehat{f}_b(x) \leq \nu \} \leq .05$, define the $95\%$ percentile analogously. 
      
        \medskip
      
        Plot the $5$ and $95$ percentile curves on the plots from part b.
        \fi
      
        \item \points{5} Repeat parts a and b with $n=300$, but use $10$-fold CV instead of leave-one-out for part a.
        \iffalse \Yuanyuan{Commented out the bootstrap part.}
        \item \points{5} For this problem, use the $\widehat{f}_{\rm poly}(x)$ and $\widehat{f}_{\rm rbf}(x)$ learned in part d. Suppose $m=1000$ additional samples $(x_1',y_1'),\dots,(x_m',y_m')$ are drawn i.i.d. the same way the first $n$ samples were drawn. 
      
        \medskip
        
        Use the non-parametric bootstrap with $B=300$ to construct a confidence interval on
        $$
        \E{\left(Y-\widehat{f}_{\rm poly}(X)\right)^2 - \left(Y-\widehat{f}_{\rm rbf}(X)\right)^2}
        $$
        (i.e. randomly draw with replacement $m$ samples denoted as $\{(\widetilde{x}_i',\widetilde{y}_i')\}_{i=1}^m$ from $\{(x_i',y_i')\}_{i=1}^m$ and compute $\frac{1}{m} \sum\limits_{i=1}^m \left[ \left(\widetilde{y}_i'-\widehat{f}_{\rm poly}(\widetilde{x}_i')\right)^2 - \left(\widetilde{y}_i'-\widehat{f}_{\rm rbf}(\widetilde{x}_i')\right)^2 \right]$, repeat this $B$ times) and find $5\%$ and $95\%$ percentiles. Report these values.
        
        \medskip
        
        Using this confidence interval, is there statistically significant evidence to suggest that one of $\widehat{f}_{\rm rbf}$ and $\widehat{f}_{\rm poly}$ is better than the other at predicting $Y$ from $X$? (Hint: does the confidence interval contain $0$?)
        \fi
        
    \end{enumerate}
    \subsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Part a:} Report the values of $d$, $\gamma$ and the value of $\lambda$ for both kernels as described.
        \item \textbf{Part b:} Two plots. One plot for each function.
        \item \textbf{Part c:} Values of $d$, $\gamma$, and the value of $\lambda$ for both kernels as described. In addition, provide two separate plots as you did for part b. 
        \item \textbf{Code} on Gradescope through coding submission.
    \end{itemize}
\end{aprob}

\iffalse
\section*{Introduction to PyTorch}
\Yuanyuan{Only include the MNIST question. Commented this one out.}
\begin{aprob}
    \label{code-pytorch}
    PyTorch is a great tool for developing, deploying and researching neural networks and other gradient-based algorithms.
    In this problem we will explore how this package is built, and re-implement some of its core components.
    Firstly start by reading \texttt{README.md} file provided in \texttt{intro\_pytorch} subfolder.
    A lot of problem statements will overlap between here, readme's and comments in functions.
    
    \begin{enumerate}
        \item \points{10} You will start by implementing components of our own PyTorch modules. You can find these in folders: \texttt{layers}, \texttt{losses} and \texttt{optimizers}. Almost each file there should contain at least one problem function, including exact directions for what to achieve in this problem. Lastly, you should implement functions in \texttt{train.py} file.
        \item \points{5} Next we will use the above module to perform hyperparameter search. Here we will also treat loss function as a hyper-parameter. However, because cross-entropy and MSE require different shapes we are going to use two different files: \texttt{crossentropy\_search.py} and \texttt{mean\_squared\_error\_search.py}.
        For each you will need to build and train (in provided order) 5 models:
        \begin{itemize}
            \item Linear neural network (Single layer, no activation function)
            \item NN with one hidden layer (2 units) and sigmoid activation function after the hidden layer
            \item NN with one hidden layer (2 units) and ReLU activation function after the hidden layer
            \item NN with two hidden layer (each with 2 units) and Sigmoid, ReLU activation functions after first and second hidden layers, respectively
            \item NN with two hidden layer (each with 2 units) and ReLU, Sigmoid activation functions after first and second hidden layers, respectively
        \end{itemize}
        For each loss function, submit a plot of losses from training and validation sets. All models should be on the same plot (10 lines per plot), with two plots total (1 for MSE, 1 for cross-entropy).
        \item \points{5} For each loss function, report the best performing architecture (best performing is defined here as achieving the lowest validation loss at any point during the training), and plot it's guesses on test set. You should use function \texttt{plot\_model\_guesses} from \texttt{train.py} file. Lastly, report accuracy of that model on a test set.
    \end{enumerate}
    
    \subsubsection*{On Softmax function}
    One of the activation functions we ask you to implement is softmax. For a prediction $\hat{y} \in \mathbb{R}^k$ corresponding to single datapoint (in a problem with $k$ classes):
    \begin{align*}
        \text{softmax}(\hat{y}_i) = \frac{\exp(\hat{y}_i)}{\sum_j \exp(\hat{y}_j)}
    \end{align*}
    
    \subsubsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Part b:} 2 plots (one per loss function), with 10 lines each, showing both training and validation loss of each model. Make sure plots are titled, and have proper legends.
        \item \textbf{Part c:} Names of best performing models (i.e. descriptions of their architectures), and their accuracy on test set.
        \item \textbf{Part c:} 2 scatter plots (one per loss function), with predictions of best performing models on test set.
        \item \textbf{Code} on Gradescope through coding submission
    \end{itemize}
\end{aprob}
\fi

\section*{Neural Networks for MNIST}
For questions A.6 you will use a lot of PyTorch.
%\Yuanyuan{Add some sentences here.}
        \textbf{This assignment will take way longer time then previous coding problems, especially for those who are not familiar with PyTorch.} We advise that students might need to start early on this problem.

    \subsection*{Resources}
        In Section materials (Week 7-8) there are iPython notebooks that you might find useful.
        Additionally make use of \href{https://pytorch.org/docs/stable/index.html}{PyTorch Documentation}, when needed.
        
        If you do not have access to GPU, you might find \href{https://colab.research.google.com/}{Google Colaboratory} useful.
        It allows you to use a cloud GPU for free.
        To enable it make sure: "Runtime" -> "Change runtime type" -> "Hardware accelerator" is set to "GPU". When submitting please download and submit a \texttt{.py} version of your notebook.


\begin{aprob}
    \label{code-nn-mnist}
    In Homework 1, we used ridge regression for training a classifier for the MNIST data set.
    %In Homework 2, we used logistic regression to distinguish between the digits 2 and 7.
    In this problem, we will use PyTorch to build a simple neural network classifier for MNIST to further improve our accuracy.\\\\
    We will implement two different architectures: a shallow but wide network, and a narrow but deeper network. For both architectures,
    we use $d$ to refer to the number of input features (in MNIST, $d=28^2 = 784$), $h_i$ to refer to the dimension of the $i$-th hidden layer and $k$ for the number of target classes (in MNIST, $k=10$). For the non-linear activation, use ReLU. Recall from lecture that
    \[ \text{ReLU}(x) = \begin{cases} 
          x, & x \geq 0 \\
          0, & x < 0 \ .
       \end{cases}
    \]
    \subsubsection*{Weight Initialization}
    Consider a weight matrix $W \in \mathbb{R}^{n \times m}$ and $b \in \mathbb{R}^n$. Note that here $m$ refers to the input dimension and
    $n$ to the output dimension of the transformation $x \mapsto Wx + b$. Define $\alpha = \frac{1}{\sqrt{m}}$.
    Initialize all your weight matrices and biases according to $\text{Unif}(-\alpha, \alpha)$.
    
    \subsubsection*{Training}
    For this assignment, use the Adam optimizer from \texttt{torch.optim}. Adam is a more advanced form of gradient descent that combines momentum and learning rate scaling. It often converges faster than regular gradient descent in practice. You can use either Gradient Descent or any form of Stochastic Gradient Descent. Note that you are still using Adam, but might pass either the full data, a single datapoint or a batch of data to it. Use cross entropy for the loss function and ReLU for the non-linearity.
    \subsubsection*{Implementing the Neural Networks}
    \begin{enumerate}
        \item \points{10}
        Let $W_0 \in \mathbb{R}^{h \times d}$, $b_0 \in \mathbb{R}^h$, $W_1 \in \mathbb{R}^{k \times h}$, $b_1 \in \mathbb{R}^k$ and $\sigma(z)\colon \mathbb{R} \to \mathbb{R}$
        some non-linear activation function applied element-wise. Given some $x \in \mathbb{R}^{d}$, the forward pass of the wide, shallow network can be formulated as:
        $$\mathcal{F}_1(x) \coloneqq W_1 \sigma(W_0 x + b_0) + b_1$$
        Use $h=64$ for the number of hidden units and choose an appropriate learning rate.
        Train the network until it reaches $99\%$ accuracy on the training data and provide a training plot (loss vs. epoch).
        Finally evaluate the model on the test data and report both the accuracy and the loss.
        \item \points{10}
        Let $W_0 \in \mathbb{R}^{h_0 \times d}$, $b_0 \in \mathbb{R}^{h_0}$, $W_1 \in \mathbb{R}^{h_1 \times h_0}$, $b_1 \in \mathbb{R}^{h_1}$,
        $W_2 \in \mathbb{R}^{k \times h_1}$, $b_2 \in \mathbb{R}^{k}$ and $\sigma(z) : \mathbb{R} \rightarrow \mathbb{R}$
        some non-linear activation function. Given some $x \in \mathbb{R}^{d}$, the forward pass of the network can be formulated as:
        $$\mathcal{F}_2(x) \coloneqq W_2 \sigma(W_1 \sigma(W_0 x + b_0) + b_1) + b_2$$
        Use $h_0 = h_1 = 32$ and perform the same steps as in part a.
        \item \points{5}
        Compute the total number of parameters of each network and report them.
        Then compare the number of parameters as well as the test accuracies the networks achieved. Is one of the approaches (wide, shallow vs. narrow, deeper) better than the other? Give
        an intuition for why or why not.
    \end{enumerate}
    
    \textcolor{red}{\textbf{Using PyTorch:}} For your solution, you may not use any functionality from the \texttt{torch.nn} module except for \texttt{torch.nn.functional.relu} and \texttt{torch.nn.functional.cross\_entropy}. You must implement the networks $\mathcal{F}_1$ and $\mathcal{F}_2$ from scratch. For starter code and a tutorial on PyTorch refer to the sections 6 and 7 material.
    \subsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Parts a-b:} Provide a plot of the training loss versus epoch. In addition evaluate the model trained on the test data and report the accuracy and loss.
        \item \textbf{Part c:} Report the number of parameters for the network trained in part (a) and for the network trained in part (b).  Provide a comparison of the two networks as described in part in 1-2 sentences.
        \item \textbf{Code} on Gradescope through coding submission.
    \end{itemize}
\end{aprob}

\section*{Intro to Sample Complexity}
\begin{bprob}
    For $i=1,\dots,n$ let $(x_i,y_i) \overset{\rm i.i.d.}{\sim} P_{X,Y}$ where $y_i \in \{-1,1\}$ and $x_i$ lives in some set $\mathcal{X}$ ($x_i$ is not necessarily a vector). 
    The $0/1$ loss, or \emph{risk}, for a deterministic classifier $f\colon \mathcal{X} \rightarrow \{ -1,1 \}$ is defined as:
    $$R(f) = \mathbb E_{X,Y} [\1(f(X)\neq Y)]$$
    where $\1(\mathcal{E})$ is the indicator function for the event $\mathcal{E}$ (the function takes the value $1$ if $\mathcal{E}$ occurs and $0$ otherwise).
    The expectation is with respect to the underlying distribution $P_{X,Y}$ on $(X,Y)$.
    Unfortunately, we don't know $P_{X,Y}$ exactly, but we do have our i.i.d. samples $\{(x_i,y_i)\}_{i=1}^n$ drawn from it.
    Define the \emph{empirical risk} as 
    $$\widehat R_n(f) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}(f(x_i)\neq y_i)\ ,$$
    which is just an empirical estimate of our risk.
    Suppose that a learning algorithm computes the empirical risk $R_n(f)$ for all $f \in \mathcal{F}$ and outputs the prediction function $\widehat f$ which is the one with the smallest empirical risk.
    (In this problem, we are assuming that  $\mathcal{F}$ is finite.)
    Suppose that the best-in-class function  $f^*$ (i.e., the one that minimizes the true 0/1 loss) is:
    $$f^* = \arg\min_{f \in \mathcal{F}} R(f) \, . $$
    \begin{enumerate}
        \item \points{2} Suppose that for some $f \in \mathcal{F}$, we have $R(f) > \epsilon$. 
        Show that
        $$\P\left[ \widehat{R}_n(f) = 0 \right] \leq \eu^{-n \epsilon}\ .$$
        (You may use the fact that $1-\epsilon \le \eu^{-\epsilon}$.)
        \item \points{2} Use the {\em union bound} to show that
        $$\P \left[\exists f \in \mathcal{F}\text{ s.t. } R(f) > \epsilon \text{ and }  \widehat{R}_n(f) = 0 \right] \le \lvert\mathcal{F}\rvert \eu^{-\epsilon n}\ .$$
        Recall that the union bound says that if $A_1, \ldots, A_k$ are events in a probability space, then 
        $$\P\left[A_1 \cup A_2 \cup \ldots \cup A_k\right] \le \sum_{1\le i \le k} \P(A_i).$$
        \item \points{2} Solve for the minimum $\epsilon$ such that
        $\lvert \mathcal{F}\rvert \eu^{-\epsilon n} \le \delta$. 
        \item \points{4} Use this to show that with probability at least $1-\delta$ 
        \begin{align*}
            \widehat{R}_n(\widehat{f})=0 \quad \implies \quad  R(\widehat f) - R\left(f^*\right) \leq \frac{\log(\lvert\mathcal{F}\rvert/\delta)}{n}
        \end{align*}
        where $\widehat f = \arg\min_{f\in\mathcal{F}} \widehat R_n(f)$.
    \end{enumerate}
    \textbf{Context:}
    Note that among a larger number of functions $\mathcal{F}$ there is more likely to exist an $\widehat{f}$ such that $\widehat{R}_n(\widehat{f})=0$. 
    However, this increased flexibility comes at the cost of a worse guarantee on the true error reflected in the larger $\lvert \mathcal{F}\rvert$. This trade-off quantifies how we can choose function classes $\mathcal{F}$ that over fit.
    This sample complexity result is remarkable because it depends just on the number of functions in $\mathcal{F}$, not what they look like. This is among the simplest results among a rich literature known as `Statistical Learning Theory'.
    Using a similar strategy, one can use Hoeffding's inequality to obtain a generalization bound when $\widehat{R}_n(\widehat{f}) \neq 0$.
    
    \subsection*{What to Submit:}
    \begin{itemize}
        \item \textbf{Part a:} A proof that $\P\left[ \widehat{R}_n(f) = 0 \right] \leq \eu^{-n \epsilon}$.
        \item \textbf{Part b:} A proof that $\P \left[\exists f \in \mathcal{F}\text{ s.t. } R(f) > \epsilon \text{ and }  \widehat{R}_n(f) = 0 \right] \le \lvert\mathcal{F}\rvert \eu^{-\epsilon n}$.
        \item \textbf{Part c:} A solution finding the minimum that satisfies the equation.
        \item \textbf{Part d:} A proof that $\widehat{R}_n(\widehat{f})=0 \quad \implies \quad  R(\widehat f) - R\left(f^*\right) \leq \frac{\log(\lvert\mathcal{F}\rvert/\delta)}{n}$.
    \end{itemize}
\end{bprob}


\end{document}
